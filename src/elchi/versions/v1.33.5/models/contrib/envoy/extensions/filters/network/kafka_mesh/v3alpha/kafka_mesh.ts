// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.7
//   protoc               unknown
// source: contrib/envoy/extensions/filters/network/kafka_mesh/v3alpha/kafka_mesh.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import { messageTypeRegistry } from "../../../../../../../typeRegistry";

export const protobufPackage = "envoy.extensions.filters.network.kafka_mesh.v3alpha";

/** [#next-free-field: 6] */
export interface KafkaMesh {
  $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaMesh";
  /**
   * Envoy's host that's advertised to clients.
   * Has the same meaning as corresponding Kafka broker properties.
   * Usually equal to filter chain's listener config, but needs to be reachable by clients
   * (so 0.0.0.0 will not work).
   */
  advertised_host?:
    | string
    | undefined;
  /** Envoy's port that's advertised to clients. */
  advertised_port?:
    | number
    | undefined;
  /** Upstream clusters this filter will connect to. */
  upstream_clusters?:
    | KafkaClusterDefinition[]
    | undefined;
  /** Rules that will decide which cluster gets which request. */
  forwarding_rules?:
    | ForwardingRule[]
    | undefined;
  /** How the consumer proxying should behave - this relates mostly to Fetch request handling. */
  consumer_proxy_mode?: KafkaMesh_ConsumerProxyMode | undefined;
}

export enum KafkaMesh_ConsumerProxyMode {
  /**
   * StatefulConsumerProxy - Records received are going to be distributed amongst downstream consumer connections.
   * In this mode Envoy uses librdkafka consumers pointing at upstream Kafka clusters, what means that these
   * consumers' position is meaningful and affects what records are received from upstream.
   * Users might want to take a look into these consumers' custom configuration to manage their auto-committing
   * capabilities, as it will impact Envoy's behaviour in case of restarts.
   */
  StatefulConsumerProxy = "StatefulConsumerProxy",
}

export function kafkaMesh_ConsumerProxyModeFromJSON(object: any): KafkaMesh_ConsumerProxyMode {
  switch (object) {
    case 0:
    case "StatefulConsumerProxy":
      return KafkaMesh_ConsumerProxyMode.StatefulConsumerProxy;
    default:
      throw new globalThis.Error("Unrecognized enum value " + object + " for enum KafkaMesh_ConsumerProxyMode");
  }
}

export function kafkaMesh_ConsumerProxyModeToJSON(object: KafkaMesh_ConsumerProxyMode): string {
  switch (object) {
    case KafkaMesh_ConsumerProxyMode.StatefulConsumerProxy:
      return "StatefulConsumerProxy";
    default:
      throw new globalThis.Error("Unrecognized enum value " + object + " for enum KafkaMesh_ConsumerProxyMode");
  }
}

export function kafkaMesh_ConsumerProxyModeToNumber(object: KafkaMesh_ConsumerProxyMode): number {
  switch (object) {
    case KafkaMesh_ConsumerProxyMode.StatefulConsumerProxy:
      return 0;
    default:
      throw new globalThis.Error("Unrecognized enum value " + object + " for enum KafkaMesh_ConsumerProxyMode");
  }
}

/** [#next-free-field: 6] */
export interface KafkaClusterDefinition {
  $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition";
  /** Cluster name. */
  cluster_name?:
    | string
    | undefined;
  /** Kafka cluster address. */
  bootstrap_servers?:
    | string
    | undefined;
  /**
   * Default number of partitions present in this cluster.
   * This is especially important for clients that do not specify partition in their payloads and depend on this value for hashing.
   * The same number of partitions is going to be used by upstream-pointing Kafka consumers for consumer proxying scenarios.
   */
  partition_count?:
    | number
    | undefined;
  /** Custom configuration passed to Kafka producer. */
  producer_config?:
    | Map<string, string>
    | undefined;
  /** Custom configuration passed to Kafka consumer. */
  consumer_config?: Map<string, string> | undefined;
}

export interface KafkaClusterDefinition_ProducerConfigEntry {
  $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition.ProducerConfigEntry";
  key: string;
  value: string;
}

export interface KafkaClusterDefinition_ConsumerConfigEntry {
  $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition.ConsumerConfigEntry";
  key: string;
  value: string;
}

export interface ForwardingRule {
  $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.ForwardingRule";
  /** Cluster name. */
  target_cluster?: string | undefined;
  trigger?:
    | //
    /** Intended place for future types of forwarding rules. */
    { $case: "topic_prefix"; topic_prefix: string }
    | undefined;
}

function createBaseKafkaMesh(): KafkaMesh {
  return { $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaMesh" };
}

export const KafkaMesh: MessageFns<KafkaMesh, "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaMesh"> = {
  $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaMesh" as const,

  encode(message: KafkaMesh, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.advertised_host !== undefined && message.advertised_host !== "") {
      writer.uint32(10).string(message.advertised_host);
    }
    if (message.advertised_port !== undefined && message.advertised_port !== 0) {
      writer.uint32(16).int32(message.advertised_port);
    }
    if (message.upstream_clusters !== undefined && message.upstream_clusters.length !== 0) {
      for (const v of message.upstream_clusters) {
        KafkaClusterDefinition.encode(v!, writer.uint32(26).fork()).join();
      }
    }
    if (message.forwarding_rules !== undefined && message.forwarding_rules.length !== 0) {
      for (const v of message.forwarding_rules) {
        ForwardingRule.encode(v!, writer.uint32(34).fork()).join();
      }
    }
    if (
      message.consumer_proxy_mode !== undefined &&
      message.consumer_proxy_mode !== KafkaMesh_ConsumerProxyMode.StatefulConsumerProxy
    ) {
      writer.uint32(40).int32(kafkaMesh_ConsumerProxyModeToNumber(message.consumer_proxy_mode));
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KafkaMesh {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKafkaMesh();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.advertised_host = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.advertised_port = reader.int32();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          if (message.upstream_clusters === undefined) {
            message.upstream_clusters = [];
          }
          const el = KafkaClusterDefinition.decode(reader, reader.uint32());
          if (el !== undefined) {
            message.upstream_clusters!.push(el);
          }
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          if (message.forwarding_rules === undefined) {
            message.forwarding_rules = [];
          }
          const el = ForwardingRule.decode(reader, reader.uint32());
          if (el !== undefined) {
            message.forwarding_rules!.push(el);
          }
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.consumer_proxy_mode = kafkaMesh_ConsumerProxyModeFromJSON(reader.int32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KafkaMesh {
    return {
      $type: KafkaMesh.$type,
      advertised_host: isSet(object.advertised_host) ? globalThis.String(object.advertised_host) : undefined,
      advertised_port: isSet(object.advertised_port) ? globalThis.Number(object.advertised_port) : undefined,
      upstream_clusters: globalThis.Array.isArray(object?.upstream_clusters)
        ? object.upstream_clusters.map((e: any) => KafkaClusterDefinition.fromJSON(e))
        : undefined,
      forwarding_rules: globalThis.Array.isArray(object?.forwarding_rules)
        ? object.forwarding_rules.map((e: any) => ForwardingRule.fromJSON(e))
        : undefined,
      consumer_proxy_mode: isSet(object.consumer_proxy_mode)
        ? kafkaMesh_ConsumerProxyModeFromJSON(object.consumer_proxy_mode)
        : undefined,
    };
  },

  toJSON(message: KafkaMesh): unknown {
    const obj: any = {};
    if (message.advertised_host !== undefined) {
      obj.advertised_host = message.advertised_host;
    }
    if (message.advertised_port !== undefined) {
      obj.advertised_port = Math.round(message.advertised_port);
    }
    if (message.upstream_clusters?.length) {
      obj.upstream_clusters = message.upstream_clusters.map((e) => KafkaClusterDefinition.toJSON(e));
    }
    if (message.forwarding_rules?.length) {
      obj.forwarding_rules = message.forwarding_rules.map((e) => ForwardingRule.toJSON(e));
    }
    if (message.consumer_proxy_mode !== undefined) {
      obj.consumer_proxy_mode = kafkaMesh_ConsumerProxyModeToJSON(message.consumer_proxy_mode);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<KafkaMesh>, I>>(base?: I): KafkaMesh {
    return KafkaMesh.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<KafkaMesh>, I>>(object: I): KafkaMesh {
    const message = createBaseKafkaMesh();
    message.advertised_host = object.advertised_host ?? undefined;
    message.advertised_port = object.advertised_port ?? undefined;
    message.upstream_clusters = object.upstream_clusters?.map((e) => KafkaClusterDefinition.fromPartial(e)) ||
      undefined;
    message.forwarding_rules = object.forwarding_rules?.map((e) => ForwardingRule.fromPartial(e)) || undefined;
    message.consumer_proxy_mode = object.consumer_proxy_mode ?? undefined;
    return message;
  },
};

messageTypeRegistry.set(KafkaMesh.$type, KafkaMesh);

function createBaseKafkaClusterDefinition(): KafkaClusterDefinition {
  return { $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition" };
}

export const KafkaClusterDefinition: MessageFns<
  KafkaClusterDefinition,
  "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition"
> = {
  $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition" as const,

  encode(message: KafkaClusterDefinition, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.cluster_name !== undefined && message.cluster_name !== "") {
      writer.uint32(10).string(message.cluster_name);
    }
    if (message.bootstrap_servers !== undefined && message.bootstrap_servers !== "") {
      writer.uint32(18).string(message.bootstrap_servers);
    }
    if (message.partition_count !== undefined && message.partition_count !== 0) {
      writer.uint32(24).int32(message.partition_count);
    }
    (message.producer_config || new Map()).forEach((value, key) => {
      KafkaClusterDefinition_ProducerConfigEntry.encode({
        $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition.ProducerConfigEntry",
        key: key as any,
        value,
      }, writer.uint32(34).fork()).join();
    });
    (message.consumer_config || new Map()).forEach((value, key) => {
      KafkaClusterDefinition_ConsumerConfigEntry.encode({
        $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition.ConsumerConfigEntry",
        key: key as any,
        value,
      }, writer.uint32(42).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KafkaClusterDefinition {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKafkaClusterDefinition();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.cluster_name = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.bootstrap_servers = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.partition_count = reader.int32();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          const entry4 = KafkaClusterDefinition_ProducerConfigEntry.decode(reader, reader.uint32());
          if (entry4.value !== undefined) {
            if (message.producer_config === undefined) {
              message.producer_config = new Map();
            }
            message.producer_config!.set(entry4.key, entry4.value);
          }
          continue;
        }
        case 5: {
          if (tag !== 42) {
            break;
          }

          const entry5 = KafkaClusterDefinition_ConsumerConfigEntry.decode(reader, reader.uint32());
          if (entry5.value !== undefined) {
            if (message.consumer_config === undefined) {
              message.consumer_config = new Map();
            }
            message.consumer_config!.set(entry5.key, entry5.value);
          }
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KafkaClusterDefinition {
    return {
      $type: KafkaClusterDefinition.$type,
      cluster_name: isSet(object.cluster_name) ? globalThis.String(object.cluster_name) : undefined,
      bootstrap_servers: isSet(object.bootstrap_servers) ? globalThis.String(object.bootstrap_servers) : undefined,
      partition_count: isSet(object.partition_count) ? globalThis.Number(object.partition_count) : undefined,
      producer_config: isObject(object.producer_config)
        ? Object.entries(object.producer_config).reduce<Map<string, string>>((acc, [key, value]) => {
          acc.set(key, String(value));
          return acc;
        }, new Map())
        : undefined,
      consumer_config: isObject(object.consumer_config)
        ? Object.entries(object.consumer_config).reduce<Map<string, string>>((acc, [key, value]) => {
          acc.set(key, String(value));
          return acc;
        }, new Map())
        : undefined,
    };
  },

  toJSON(message: KafkaClusterDefinition): unknown {
    const obj: any = {};
    if (message.cluster_name !== undefined) {
      obj.cluster_name = message.cluster_name;
    }
    if (message.bootstrap_servers !== undefined) {
      obj.bootstrap_servers = message.bootstrap_servers;
    }
    if (message.partition_count !== undefined) {
      obj.partition_count = Math.round(message.partition_count);
    }
    if (message.producer_config?.size) {
      obj.producer_config = {};
      message.producer_config.forEach((v, k) => {
        obj.producer_config[k] = v;
      });
    }
    if (message.consumer_config?.size) {
      obj.consumer_config = {};
      message.consumer_config.forEach((v, k) => {
        obj.consumer_config[k] = v;
      });
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<KafkaClusterDefinition>, I>>(base?: I): KafkaClusterDefinition {
    return KafkaClusterDefinition.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<KafkaClusterDefinition>, I>>(object: I): KafkaClusterDefinition {
    const message = createBaseKafkaClusterDefinition();
    message.cluster_name = object.cluster_name ?? undefined;
    message.bootstrap_servers = object.bootstrap_servers ?? undefined;
    message.partition_count = object.partition_count ?? undefined;
    message.producer_config = (object.producer_config === undefined || object.producer_config === null)
      ? undefined
      : (() => {
        const m = new Map();
        (object.producer_config as Map<string, string> ?? new Map()).forEach((value, key) => {
          if (value !== undefined) {
            m.set(key, globalThis.String(value));
          }
        });
        return m;
      })();
    message.consumer_config = (object.consumer_config === undefined || object.consumer_config === null)
      ? undefined
      : (() => {
        const m = new Map();
        (object.consumer_config as Map<string, string> ?? new Map()).forEach((value, key) => {
          if (value !== undefined) {
            m.set(key, globalThis.String(value));
          }
        });
        return m;
      })();
    return message;
  },
};

messageTypeRegistry.set(KafkaClusterDefinition.$type, KafkaClusterDefinition);

function createBaseKafkaClusterDefinition_ProducerConfigEntry(): KafkaClusterDefinition_ProducerConfigEntry {
  return {
    $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition.ProducerConfigEntry",
    key: "",
    value: "",
  };
}

export const KafkaClusterDefinition_ProducerConfigEntry: MessageFns<
  KafkaClusterDefinition_ProducerConfigEntry,
  "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition.ProducerConfigEntry"
> = {
  $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition.ProducerConfigEntry" as const,

  encode(message: KafkaClusterDefinition_ProducerConfigEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KafkaClusterDefinition_ProducerConfigEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKafkaClusterDefinition_ProducerConfigEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KafkaClusterDefinition_ProducerConfigEntry {
    return {
      $type: KafkaClusterDefinition_ProducerConfigEntry.$type,
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: KafkaClusterDefinition_ProducerConfigEntry): unknown {
    const obj: any = {};
    if (message.key !== undefined) {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<KafkaClusterDefinition_ProducerConfigEntry>, I>>(
    base?: I,
  ): KafkaClusterDefinition_ProducerConfigEntry {
    return KafkaClusterDefinition_ProducerConfigEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<KafkaClusterDefinition_ProducerConfigEntry>, I>>(
    object: I,
  ): KafkaClusterDefinition_ProducerConfigEntry {
    const message = createBaseKafkaClusterDefinition_ProducerConfigEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

messageTypeRegistry.set(KafkaClusterDefinition_ProducerConfigEntry.$type, KafkaClusterDefinition_ProducerConfigEntry);

function createBaseKafkaClusterDefinition_ConsumerConfigEntry(): KafkaClusterDefinition_ConsumerConfigEntry {
  return {
    $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition.ConsumerConfigEntry",
    key: "",
    value: "",
  };
}

export const KafkaClusterDefinition_ConsumerConfigEntry: MessageFns<
  KafkaClusterDefinition_ConsumerConfigEntry,
  "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition.ConsumerConfigEntry"
> = {
  $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.KafkaClusterDefinition.ConsumerConfigEntry" as const,

  encode(message: KafkaClusterDefinition_ConsumerConfigEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): KafkaClusterDefinition_ConsumerConfigEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseKafkaClusterDefinition_ConsumerConfigEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): KafkaClusterDefinition_ConsumerConfigEntry {
    return {
      $type: KafkaClusterDefinition_ConsumerConfigEntry.$type,
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: KafkaClusterDefinition_ConsumerConfigEntry): unknown {
    const obj: any = {};
    if (message.key !== undefined) {
      obj.key = message.key;
    }
    if (message.value !== undefined) {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<KafkaClusterDefinition_ConsumerConfigEntry>, I>>(
    base?: I,
  ): KafkaClusterDefinition_ConsumerConfigEntry {
    return KafkaClusterDefinition_ConsumerConfigEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<KafkaClusterDefinition_ConsumerConfigEntry>, I>>(
    object: I,
  ): KafkaClusterDefinition_ConsumerConfigEntry {
    const message = createBaseKafkaClusterDefinition_ConsumerConfigEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

messageTypeRegistry.set(KafkaClusterDefinition_ConsumerConfigEntry.$type, KafkaClusterDefinition_ConsumerConfigEntry);

function createBaseForwardingRule(): ForwardingRule {
  return { $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.ForwardingRule", trigger: undefined };
}

export const ForwardingRule: MessageFns<
  ForwardingRule,
  "envoy.extensions.filters.network.kafka_mesh.v3alpha.ForwardingRule"
> = {
  $type: "envoy.extensions.filters.network.kafka_mesh.v3alpha.ForwardingRule" as const,

  encode(message: ForwardingRule, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.target_cluster !== undefined && message.target_cluster !== "") {
      writer.uint32(10).string(message.target_cluster);
    }
    switch (message.trigger?.$case) {
      case "topic_prefix":
        writer.uint32(18).string(message.trigger.topic_prefix);
        break;
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ForwardingRule {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseForwardingRule();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.target_cluster = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.trigger = { $case: "topic_prefix", topic_prefix: reader.string() };
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ForwardingRule {
    return {
      $type: ForwardingRule.$type,
      target_cluster: isSet(object.target_cluster) ? globalThis.String(object.target_cluster) : undefined,
      trigger: isSet(object.topic_prefix)
        ? { $case: "topic_prefix", topic_prefix: globalThis.String(object.topic_prefix) }
        : undefined,
    };
  },

  toJSON(message: ForwardingRule): unknown {
    const obj: any = {};
    if (message.target_cluster !== undefined) {
      obj.target_cluster = message.target_cluster;
    }
    if (message.trigger?.$case === "topic_prefix") {
      obj.topic_prefix = message.trigger.topic_prefix;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ForwardingRule>, I>>(base?: I): ForwardingRule {
    return ForwardingRule.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ForwardingRule>, I>>(object: I): ForwardingRule {
    const message = createBaseForwardingRule();
    message.target_cluster = object.target_cluster ?? undefined;
    if (
      object.trigger?.$case === "topic_prefix" &&
      object.trigger?.topic_prefix !== undefined &&
      object.trigger?.topic_prefix !== null
    ) {
      message.trigger = { $case: "topic_prefix", topic_prefix: object.trigger.topic_prefix };
    }
    return message;
  },
};

messageTypeRegistry.set(ForwardingRule.$type, ForwardingRule);

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends { $case: string } ? { [K in keyof Omit<T, "$case">]?: DeepPartial<T[K]> } & { $case: T["$case"] }
  : T extends {} ? { [K in Exclude<keyof T, "$type">]?: DeepPartial<T[K]> }
  : Partial<T>;

type KeysOfUnion<T> = T extends T ? keyof T : never;
export type Exact<P, I extends P> = P extends Builtin ? P
  : P & { [K in keyof P]: Exact<P[K], I[K]> } & { [K in Exclude<keyof I, KeysOfUnion<P> | "$type">]: never };

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T, V extends string> {
  readonly $type: V;
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create<I extends Exact<DeepPartial<T>, I>>(base?: I): T;
  fromPartial<I extends Exact<DeepPartial<T>, I>>(object: I): T;
}
