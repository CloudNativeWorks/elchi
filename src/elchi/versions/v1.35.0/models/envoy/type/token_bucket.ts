// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.2.7
//   protoc               unknown
// source: envoy/type/token_bucket.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import { Duration } from "../../google/protobuf/duration";
import { UInt32Value } from "../../google/protobuf/wrappers";
import { messageTypeRegistry } from "../../typeRegistry";

export const protobufPackage = "envoy.type";

/** Configures a token bucket, typically used for rate limiting. */
export interface TokenBucket {
  $type: "envoy.type.TokenBucket";
  /**
   * The maximum tokens that the bucket can hold. This is also the number of tokens that the bucket
   * initially contains.
   */
  max_tokens?:
    | number
    | undefined;
  /**
   * The number of tokens added to the bucket during each fill interval. If not specified, defaults
   * to a single token.
   */
  tokens_per_fill?:
    | number
    | undefined;
  /**
   * The fill interval that tokens are added to the bucket. During each fill interval
   * `tokens_per_fill` are added to the bucket. The bucket will never contain more than
   * `max_tokens` tokens.
   */
  fill_interval?: Duration | undefined;
}

function createBaseTokenBucket(): TokenBucket {
  return { $type: "envoy.type.TokenBucket" };
}

export const TokenBucket: MessageFns<TokenBucket, "envoy.type.TokenBucket"> = {
  $type: "envoy.type.TokenBucket" as const,

  encode(message: TokenBucket, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.max_tokens !== undefined && message.max_tokens !== 0) {
      writer.uint32(8).uint32(message.max_tokens);
    }
    if (message.tokens_per_fill !== undefined) {
      UInt32Value.encode(
        { $type: "google.protobuf.UInt32Value", value: message.tokens_per_fill! },
        writer.uint32(18).fork(),
      ).join();
    }
    if (message.fill_interval !== undefined) {
      Duration.encode(message.fill_interval, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TokenBucket {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    let end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTokenBucket();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.max_tokens = reader.uint32();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.tokens_per_fill = UInt32Value.decode(reader, reader.uint32()).value;
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.fill_interval = Duration.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TokenBucket {
    return {
      $type: TokenBucket.$type,
      max_tokens: isSet(object.max_tokens) ? globalThis.Number(object.max_tokens) : undefined,
      tokens_per_fill: isSet(object.tokens_per_fill) ? Number(object.tokens_per_fill) : undefined,
      fill_interval: isSet(object.fill_interval) ? Duration.fromJSON(object.fill_interval) : undefined,
    };
  },

  toJSON(message: TokenBucket): unknown {
    const obj: any = {};
    if (message.max_tokens !== undefined) {
      obj.max_tokens = Math.round(message.max_tokens);
    }
    if (message.tokens_per_fill !== undefined) {
      obj.tokens_per_fill = message.tokens_per_fill;
    }
    if (message.fill_interval !== undefined) {
      obj.fill_interval = Duration.toJSON(message.fill_interval);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TokenBucket>, I>>(base?: I): TokenBucket {
    return TokenBucket.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TokenBucket>, I>>(object: I): TokenBucket {
    const message = createBaseTokenBucket();
    message.max_tokens = object.max_tokens ?? undefined;
    message.tokens_per_fill = object.tokens_per_fill ?? undefined;
    message.fill_interval = (object.fill_interval !== undefined && object.fill_interval !== null)
      ? Duration.fromPartial(object.fill_interval)
      : undefined;
    return message;
  },
};

messageTypeRegistry.set(TokenBucket.$type, TokenBucket);

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends { $case: string } ? { [K in keyof Omit<T, "$case">]?: DeepPartial<T[K]> } & { $case: T["$case"] }
  : T extends {} ? { [K in Exclude<keyof T, "$type">]?: DeepPartial<T[K]> }
  : Partial<T>;

type KeysOfUnion<T> = T extends T ? keyof T : never;
export type Exact<P, I extends P> = P extends Builtin ? P
  : P & { [K in keyof P]: Exact<P[K], I[K]> } & { [K in Exclude<keyof I, KeysOfUnion<P> | "$type">]: never };

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T, V extends string> {
  readonly $type: V;
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create<I extends Exact<DeepPartial<T>, I>>(base?: I): T;
  fromPartial<I extends Exact<DeepPartial<T>, I>>(object: I): T;
}
